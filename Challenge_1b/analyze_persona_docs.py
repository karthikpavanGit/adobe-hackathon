"""
Persona-Driven Document Intelligence - Challenge 1b Solution

This script acts as an intelligent document analyst that understands what matters to specific users.
Given a collection of PDFs and a persona with a specific job-to-be-done, it extracts and ranks
the most relevant sections and insights, making documents speak directly to user needs.

Approach:
1. Extract text blocks from all PDFs using PyMuPDF
2. Generate semantic embeddings for text blocks using sentence transformers
3. Create a query embedding from persona + task description
4. Compute semantic similarity between document sections and user needs
5. Rank and select the most relevant sections and subsections
6. Output structured JSON with metadata, ranked sections, and refined text

Usage:
    python analyze_persona_docs.py --input_json input.json --pdf_dir pdfs/ --output_json output.json

Requirements:
    - PyMuPDF (fitz)
    - sentence-transformers (all-MiniLM-L6-v2)
    - numpy
    - Python 3.7+
    - No internet access required (offline operation)
    - CPU-only operation

Performance:
    - Processes 3-5 documents in < 60 seconds
    - Model size < 1GB (80MB actual)
    - Memory usage < 500MB
    - Works offline with no external dependencies
"""

import os
import json
import fitz  # PyMuPDF
import numpy as np
from datetime import datetime
from typing import List, Dict, Any

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    raise ImportError("Please install sentence-transformers: pip install sentence-transformers")

# --- CONFIGURATION ---
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"  # ~80MB, CPU-friendly, state-of-the-art performance


def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:
    """
    Extracts text blocks from a PDF, returning a list of dicts with page, block, and text.
    
    This function processes each page of the PDF and extracts meaningful text blocks
    that can be analyzed for relevance to the user's persona and task.
    
    Args:
        pdf_path: Path to the PDF file
        
    Returns:
        List of dictionaries containing document name, page number, and text content
    """
    doc = fitz.open(pdf_path)
    blocks = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        
        # Extract text blocks with their metadata
        for block in page.get_text("blocks"):
            text = block[4].strip()
            
            # Filter out very short or noisy blocks
            if text and len(text) > 20:
                blocks.append({
                    "document": os.path.basename(pdf_path),
                    "page": page_num + 1,
                    "text": text
                })
    
    return blocks


def embed_text_blocks(text_blocks: List[Dict[str, Any]], model) -> np.ndarray:
    """
    Embeds each text block using the provided model. Returns a numpy array.
    
    This function converts text into numerical representations (embeddings) that
    capture the semantic meaning of the content, enabling similarity comparisons.
    
    Args:
        text_blocks: List of text block dictionaries
        model: Sentence transformer model for embedding generation
        
    Returns:
        Numpy array of embeddings, one per text block
    """
    texts = [block["text"] for block in text_blocks]
    
    # Generate embeddings with optimized settings for CPU
    embeddings = model.encode(
        texts, 
        show_progress_bar=False, 
        convert_to_numpy=True, 
        normalize_embeddings=True
    )
    
    return embeddings


def embed_persona_task(persona: str, job: str, model) -> np.ndarray:
    """
    Embeds the persona and job-to-be-done as a single query vector.
    
    This function creates a semantic representation of what the user is looking for,
    combining their role/persona with their specific task or goal.
    
    Args:
        persona: User's role or persona (e.g., "PhD Researcher in Computational Biology")
        job: Specific task or job-to-be-done (e.g., "Prepare literature review")
        model: Sentence transformer model for embedding generation
        
    Returns:
        Numpy array representing the combined persona + task embedding
    """
    # Combine persona and task into a single query
    query = f"Persona: {persona}. Task: {job}"
    
    # Generate embedding for the query
    embedding = model.encode(
        [query], 
        show_progress_bar=False, 
        convert_to_numpy=True, 
        normalize_embeddings=True
    )[0]
    
    return embedding


def rank_by_relevance(text_blocks: List[Dict[str, Any]], block_embeddings: np.ndarray, 
                     persona_embedding: np.ndarray, top_n_sections=5, top_n_subsections=5) -> tuple:
    """
    Ranks text blocks by cosine similarity to the persona/task embedding.
    Returns top N sections and subsections.
    
    This function computes how relevant each text block is to the user's needs
    and selects the most relevant content for the final output.
    
    Args:
        text_blocks: List of text block dictionaries
        block_embeddings: Embeddings for all text blocks
        persona_embedding: Embedding of the persona + task query
        top_n_sections: Number of top sections to return
        top_n_subsections: Number of top subsections to return
        
    Returns:
        Tuple of (sections, subsections) lists with ranked content
    """
    # Compute cosine similarity between all blocks and the persona/task
    scores = np.dot(block_embeddings, persona_embedding)
    
    # Get indices of blocks ranked by similarity (highest first)
    ranked_indices = np.argsort(scores)[::-1]
    
    sections = []
    subsections = []
    used_titles = set()  # Avoid duplicate section titles
    
    # Extract top sections
    for rank, idx in enumerate(ranked_indices[:top_n_sections]):
        block = text_blocks[idx]
        
        # Use first line as section title (truncate if too long)
        section_title = block["text"].split("\n")[0][:80]
        
        # Skip if we've already used this title
        if section_title.lower() in used_titles:
            continue
            
        used_titles.add(section_title.lower())
        
        sections.append({
            "document": block["document"],
            "section_title": section_title,
            "importance_rank": rank + 1,
            "page_number": block["page"]
        })
    
    # Extract top subsections
    for rank, idx in enumerate(ranked_indices[:top_n_subsections]):
        block = text_blocks[idx]
        
        # Truncate text for brevity while preserving meaning
        refined_text = block["text"][:500]
        
        subsections.append({
            "document": block["document"],
            "refined_text": refined_text,
            "page_number": block["page"],
            "importance_rank": rank + 1
        })
    
    return sections, subsections


def generate_output_json(input_documents: List[str], persona: str, job: str, 
                        sections: List[Dict[str, Any]], subsections: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Generates the final output JSON with metadata, extracted sections, and subsection analysis.
    
    This function creates a structured output that includes all the information
    needed to understand what was processed and what insights were extracted.
    
    Args:
        input_documents: List of input PDF filenames
        persona: User's persona/role
        job: Job-to-be-done description
        sections: List of extracted sections with rankings
        subsections: List of extracted subsections with refined text
        
    Returns:
        Dictionary containing the complete analysis output
    """
    return {
        "metadata": {
            "input_documents": input_documents,
            "persona": persona,
            "job_to_be_done": job,
            "processing_timestamp": datetime.utcnow().isoformat()
        },
        "extracted_sections": sections,
        "subsection_analysis": subsections
    }


def main(input_json_path: str, pdf_dir: str, output_json_path: str):
    """
    Main pipeline for persona-driven document analysis.
    
    This function orchestrates the complete analysis process:
    1. Load and parse the input configuration
    2. Extract text from all PDFs
    3. Generate embeddings and compute similarities
    4. Rank and select relevant content
    5. Generate structured output
    
    Args:
        input_json_path: Path to the input JSON file with persona and task
        pdf_dir: Directory containing the PDF files to analyze
        output_json_path: Path where the output JSON will be saved
    """
    print("=" * 60)
    print("Persona-Driven Document Intelligence")
    print("Challenge 1b - Adobe India Hackathon 2025")
    print("=" * 60)
    
    # Load input JSON configuration
    print("Loading input configuration...")
    with open(input_json_path, "r", encoding="utf-8") as f:
        input_data = json.load(f)
    
    persona = input_data["persona"]["role"]
    job = input_data["job_to_be_done"]["task"]
    pdf_files = [doc["filename"] for doc in input_data["documents"]]
    pdf_paths = [os.path.join(pdf_dir, fname) for fname in pdf_files]
    
    print(f"Persona: {persona}")
    print(f"Task: {job}")
    print(f"Documents: {len(pdf_files)} PDFs")
    print()
    
    # Extract text blocks from all PDFs
    print("Extracting text from PDFs...")
    all_blocks = []
    
    for pdf_path in pdf_paths:
        if not os.path.exists(pdf_path):
            print(f"‚ö†Ô∏è  Warning: PDF not found: {pdf_path}")
            continue
            
        try:
            blocks = extract_text_from_pdf(pdf_path)
            all_blocks.extend(blocks)
            print(f"‚úì Extracted {len(blocks)} text blocks from {os.path.basename(pdf_path)}")
        except Exception as e:
            print(f"‚úó Error extracting from {pdf_path}: {e}")
    
    if not all_blocks:
        print("‚ùå No text blocks extracted. Exiting.")
        return
    
    print(f"Total text blocks extracted: {len(all_blocks)}")
    print()
    
    # Load the embedding model
    print("Loading semantic embedding model...")
    model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print(f"‚úì Model loaded: {EMBEDDING_MODEL_NAME}")
    print()
    
    # Generate embeddings for text blocks and persona/task
    print("Generating semantic embeddings...")
    block_embeddings = embed_text_blocks(all_blocks, model)
    persona_embedding = embed_persona_task(persona, job, model)
    print("‚úì Embeddings generated successfully")
    print()
    
    # Rank and select top sections/subsections
    print("Ranking content by relevance...")
    sections, subsections = rank_by_relevance(all_blocks, block_embeddings, persona_embedding)
    print(f"‚úì Selected {len(sections)} top sections and {len(subsections)} subsections")
    print()
    
    # Generate and save output JSON
    print("Generating output...")
    output = generate_output_json(pdf_files, persona, job, sections, subsections)
    
    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"‚úì Output written to: {output_json_path}")
    print()
    print("üéâ Analysis completed successfully!")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Persona-Driven PDF Document Analysis",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze travel planning documents
  python analyze_persona_docs.py \\
    --input_json Collection\\ 1/challenge1b_input.json \\
    --pdf_dir Collection\\ 1/PDFs \\
    --output_json Collection\\ 1/challenge1b_output.json
        """
    )
    
    parser.add_argument(
        "--input_json", 
        required=True, 
        help="Path to input JSON file with persona and task configuration"
    )
    parser.add_argument(
        "--pdf_dir", 
        required=True, 
        help="Directory containing PDF files to analyze"
    )
    parser.add_argument(
        "--output_json", 
        required=True, 
        help="Path where the output JSON file will be saved"
    )
    
    args = parser.parse_args()
    main(args.input_json, args.pdf_dir, args.output_json) 